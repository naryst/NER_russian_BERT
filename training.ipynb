{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !gdown 1FbAhm8H7MtPN_CJj-kYuI7jy_CRQsuys"
   ],
   "metadata": {
    "id": "M1XHxJhZE2gD",
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.928907Z",
     "end_time": "2023-04-18T17:59:25.020001Z"
    }
   },
   "execution_count": 120,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !unzip -n data.zip "
   ],
   "metadata": {
    "id": "CmiFfnxTFHPs",
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.970731Z",
     "end_time": "2023-04-18T17:59:25.020320Z"
    }
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install transformers --quiet"
   ],
   "metadata": {
    "id": "e49naqIFFKZW",
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.970801Z",
     "end_time": "2023-04-18T17:59:25.020511Z"
    }
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.970851Z",
     "end_time": "2023-04-18T17:59:25.020624Z"
    },
    "id": "2dL8xO4szeid"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "ROOT_DIR = \"train_data/\"\n",
    "texts50 = ROOT_DIR + \"50texts_tacred/\"\n",
    "cll3 = ROOT_DIR + \"coll3_tacred/\"\n",
    "legal = ROOT_DIR + \"legal/\"\n",
    "tacred2 = ROOT_DIR + \"tacred2/\"\n",
    "tacred3 = ROOT_DIR + \"tacred3/\""
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.970914Z",
     "end_time": "2023-04-18T17:59:25.020703Z"
    },
    "id": "kalyOhTgzeih"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def create_dataframe(dir):\n",
    "    df = pd.DataFrame(columns=[\"fName\", \"text\", \"entities\"])\n",
    "    df = df.set_index(\"fName\")\n",
    "    files = os.listdir(dir)\n",
    "    for name in files:\n",
    "        fname, ext = name.split(\".\")\n",
    "        with open(dir + name) as f:\n",
    "            text = f.read()\n",
    "        if ext == \"txt\":\n",
    "            ann_fname = files[files.index(fname + \".ann\")]\n",
    "            with open(dir + ann_fname) as f_ann:\n",
    "                entities = f_ann.readlines()\n",
    "            new_entities = []\n",
    "            for i in range(len(entities)):\n",
    "                entity_type = entities[i][0]\n",
    "                entities[i] = entities[i][:-1]\n",
    "                if entity_type == \"T\":\n",
    "                    entities[i] = entities[i].split(\"\\t\")\n",
    "                    entities[i] = entities[i][1:]\n",
    "                    src = entities[i][1]\n",
    "                    entities[i] = entities[i][0].split()\n",
    "                    entities[i].append(src)\n",
    "                    assert len(entities[i]) <= 5\n",
    "                    if len(entities[i]) == 5:\n",
    "                        begin = entities[i][1]\n",
    "                        inter1, inter2 = entities[i][2].split(\";\")\n",
    "                        end = entities[i][3]\n",
    "                        new_entities.append(\n",
    "                            [entities[i][0], begin, inter1, entities[i][4]]\n",
    "                        )\n",
    "                        new_entities.append(\n",
    "                            [entities[i][0], inter2, end, entities[i][4]]\n",
    "                        )\n",
    "                    else:\n",
    "                        new_entities.append(entities[i])\n",
    "            df.loc[fname] = [text, new_entities]\n",
    "    return df"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.970978Z",
     "end_time": "2023-04-18T17:59:25.020783Z"
    },
    "id": "UlS-SEorzeij"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "df1 = create_dataframe(texts50)\n",
    "df2 = create_dataframe(cll3)\n",
    "df3 = create_dataframe(legal)\n",
    "df4 = create_dataframe(tacred2)\n",
    "df5 = create_dataframe(tacred3)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:24.971108Z",
     "end_time": "2023-04-18T17:59:27.067977Z"
    },
    "id": "Y5dP52Fazeim",
    "outputId": "fd1e9d64-3c45-4a7a-dfc7-a9929422d1a6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "841"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape[0] + df2.shape[0] + df3.shape[0] + df4.shape[0] + df5.shape[0]"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.073216Z",
     "end_time": "2023-04-18T17:59:27.076372Z"
    },
    "id": "Oo5nCp4izeio"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "total_df = pd.concat([df1, df2, df3, df4, df5])"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.079017Z",
     "end_time": "2023-04-18T17:59:27.081042Z"
    },
    "id": "0lggtpAfzeip"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.082151Z",
     "end_time": "2023-04-18T17:59:27.120932Z"
    },
    "id": "5a1plEEozeir"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Evolett/rubert-tiny2-finetuned-ner\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.099662Z",
     "end_time": "2023-04-18T17:59:27.931221Z"
    },
    "id": "Cn5DkX3Xzeir"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "def identify_tokens(df_sample):\n",
    "    sample_text = df_sample[\"text\"]\n",
    "    sample_en = df_sample[\"entities\"]\n",
    "    sample_text_tokenized = tokenizer.tokenize(sample_text)\n",
    "    trash_symbols_count = []\n",
    "    for i in range(len(sample_text)):\n",
    "        cur = (\n",
    "            1\n",
    "            if (\n",
    "                sample_text[i] == \" \"\n",
    "                or sample_text[i] == \"\\n\"\n",
    "                or sample_text[i] == \"\\xa0\"\n",
    "            )\n",
    "            else 0\n",
    "        )\n",
    "        if i == 0:\n",
    "            trash_symbols_count.append(cur)\n",
    "        else:\n",
    "            trash_symbols_count.append(trash_symbols_count[-1] + cur)\n",
    "    cur_entity_id = 0\n",
    "    processed_symbols = 0\n",
    "    mappings = []\n",
    "    begin = int(sample_en[cur_entity_id][1])\n",
    "    end = int(sample_en[cur_entity_id][2])\n",
    "    assert end < len(trash_symbols_count)\n",
    "    begin -= trash_symbols_count[begin]\n",
    "    end -= trash_symbols_count[end]\n",
    "\n",
    "    for token in sample_text_tokenized:\n",
    "        while processed_symbols > end and (cur_entity_id + 1) < len(sample_en):\n",
    "            cur_entity_id += 1\n",
    "            begin = int(sample_en[cur_entity_id][1])\n",
    "            end = int(sample_en[cur_entity_id][2]) - 1\n",
    "            begin -= trash_symbols_count[begin]\n",
    "            end -= trash_symbols_count[end]\n",
    "        if begin <= processed_symbols <= end:\n",
    "            first_token = not (\n",
    "                len(mappings) != 0 and sample_en[cur_entity_id] == mappings[-1][1]\n",
    "            )\n",
    "            mappings.append((token, sample_en[cur_entity_id], first_token))\n",
    "        else:\n",
    "            mappings.append((token, False))\n",
    "        subword_check = token[:2] == '##'\n",
    "        processed_symbols = processed_symbols + len(token) - int(subword_check) * 2\n",
    "\n",
    "    for i in range(len(mappings)):\n",
    "        if len(mappings[i]) == 2:\n",
    "            mappings[i] = \"O\"\n",
    "        else:\n",
    "            if mappings[i][-1]:\n",
    "                mappings[i] = \"B-\" + mappings[i][1][0]\n",
    "            else:\n",
    "                mappings[i] = \"I-\" + mappings[i][1][0]\n",
    "    mappings.insert(0, \"O\")  # start token\n",
    "    mappings.append(\"O\")  # end token\n",
    "    return mappings"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.938615Z",
     "end_time": "2023-04-18T17:59:27.942260Z"
    },
    "id": "WJWc-zhKzeis"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2392 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "total_df[\"tokenized_text\"] = total_df[\"text\"].apply(tokenizer.tokenize)\n",
    "total_df[\"tokenizer_output\"] = total_df[\"text\"].apply(\n",
    "    lambda x: tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
    ")\n",
    "\n",
    "# add start and end tokens to map tokenizer output\n",
    "total_df[\"tokenized_text\"] = total_df[\"tokenized_text\"].apply(\n",
    "    lambda x: [\"<s>\"] + x + [\"</s>\"]\n",
    ")\n",
    "total_df[\"classified_tokens\"] = total_df.apply(identify_tokens, axis=1)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:27.943240Z",
     "end_time": "2023-04-18T17:59:32.731958Z"
    },
    "id": "qbbneyatzeiw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "         fName                                               text  \\\n0  104189_text  Полиция Бельгии арестовала двух человек по под...   \n1  119336_text  Семья Обамы приобрела дом в Вашингтоне за 8,1 ...   \n2   11766_text  Скончался бывший премьер-министр России Виктор...   \n3   11899_text  Странное преступление против семьи россиян в А...   \n4  120890_text  Важные перемены в жизни рэпера Jay Z: Он помен...   \n\n                                            entities  \\\n0  [[ORGANIZATION, 0, 15, Полиция Бельгии], [COUN...   \n1  [[PERSON, 6, 11, Обамы], [CITY, 28, 38, Вашинг...   \n2  [[PROFESSION, 17, 32, премьер-министр], [COUNT...   \n3  [[NATIONALITY, 35, 42, россиян], [COUNTRY, 45,...   \n4  [[PERSON, 31, 36, Jay Z], [PERSON, 62, 67, JAY...   \n\n                                      tokenized_text  \\\n0  [<s>, Полиция, Бельгии, арестовал, ##а, двух, ...   \n1  [<s>, Семья, Обамы, приобрела, дом, в, Вашингт...   \n2  [<s>, Скончался, бывший, премьер, -, министр, ...   \n3  [<s>, Странно, ##е, преступление, против, семь...   \n4  [<s>, Важ, ##ные, перемены, в, жизни, рэпера, ...   \n\n                              tokenizer_output  \\\n0  [input_ids, token_type_ids, attention_mask]   \n1  [input_ids, token_type_ids, attention_mask]   \n2  [input_ids, token_type_ids, attention_mask]   \n3  [input_ids, token_type_ids, attention_mask]   \n4  [input_ids, token_type_ids, attention_mask]   \n\n                                   classified_tokens  \n0  [O, B-ORGANIZATION, I-ORGANIZATION, O, O, B-NU...  \n1  [O, O, B-PERSON, O, O, O, B-CITY, O, B-MONEY, ...  \n2  [O, O, O, B-PROFESSION, I-PROFESSION, I-PROFES...  \n3  [O, O, O, O, O, O, B-NATIONALITY, O, B-COUNTRY...  \n4  [O, O, O, O, O, O, O, B-PERSON, I-PERSON, I-PE...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fName</th>\n      <th>text</th>\n      <th>entities</th>\n      <th>tokenized_text</th>\n      <th>tokenizer_output</th>\n      <th>classified_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>104189_text</td>\n      <td>Полиция Бельгии арестовала двух человек по под...</td>\n      <td>[[ORGANIZATION, 0, 15, Полиция Бельгии], [COUN...</td>\n      <td>[&lt;s&gt;, Полиция, Бельгии, арестовал, ##а, двух, ...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n      <td>[O, B-ORGANIZATION, I-ORGANIZATION, O, O, B-NU...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>119336_text</td>\n      <td>Семья Обамы приобрела дом в Вашингтоне за 8,1 ...</td>\n      <td>[[PERSON, 6, 11, Обамы], [CITY, 28, 38, Вашинг...</td>\n      <td>[&lt;s&gt;, Семья, Обамы, приобрела, дом, в, Вашингт...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n      <td>[O, O, B-PERSON, O, O, O, B-CITY, O, B-MONEY, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11766_text</td>\n      <td>Скончался бывший премьер-министр России Виктор...</td>\n      <td>[[PROFESSION, 17, 32, премьер-министр], [COUNT...</td>\n      <td>[&lt;s&gt;, Скончался, бывший, премьер, -, министр, ...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n      <td>[O, O, O, B-PROFESSION, I-PROFESSION, I-PROFES...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11899_text</td>\n      <td>Странное преступление против семьи россиян в А...</td>\n      <td>[[NATIONALITY, 35, 42, россиян], [COUNTRY, 45,...</td>\n      <td>[&lt;s&gt;, Странно, ##е, преступление, против, семь...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n      <td>[O, O, O, O, O, O, B-NATIONALITY, O, B-COUNTRY...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>120890_text</td>\n      <td>Важные перемены в жизни рэпера Jay Z: Он помен...</td>\n      <td>[[PERSON, 31, 36, Jay Z], [PERSON, 62, 67, JAY...</td>\n      <td>[&lt;s&gt;, Важ, ##ные, перемены, в, жизни, рэпера, ...</td>\n      <td>[input_ids, token_type_ids, attention_mask]</td>\n      <td>[O, O, O, O, O, O, O, B-PERSON, I-PERSON, I-PE...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.reset_index(inplace=True)\n",
    "total_df.head()"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:32.740915Z",
     "end_time": "2023-04-18T17:59:32.803654Z"
    },
    "id": "PBeppkmIzeix"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_df[\"tokenized_text\"].apply(len).hist(bins=150)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:32.800914Z",
     "end_time": "2023-04-18T17:59:33.094393Z"
    },
    "id": "JcncVFK7zeiz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train dataset, which have more tokens, when BERT can process - 0.119%\n"
     ]
    }
   ],
   "source": [
    "lens = total_df[\"classified_tokens\"].apply(len)\n",
    "print(\n",
    "    f\"Number of samples in train dataset, which have more tokens, when BERT can process - {lens[lens > 2048].count() / lens.count() * 100:.3f}%\"\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-18T17:59:33.095270Z",
     "end_time": "2023-04-18T17:59:33.098572Z"
    },
    "id": "yLTQLyGkzei0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset have 0.11% of samples, what can't be processed by BERT(For example, in document '1109' input text tokenized at more than 2392 tokens). To deal with this problem, we can use sliding window approach. But in our case, number of samples with >2038 tokens in insignificant, so we can pay no attention to this."
   ],
   "metadata": {
    "collapsed": false,
    "id": "S1Zc_5BMzei1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_df[\"input_ids\"] = total_df[\"tokenizer_output\"].apply(lambda x: x[\"input_ids\"])\n",
    "total_df[\"attention_mask\"] = total_df[\"tokenizer_output\"].apply(\n",
    "    lambda x: x[\"attention_mask\"]\n",
    ")"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.369479Z",
     "end_time": "2023-04-15T20:59:46.374991Z"
    },
    "id": "hT7nK7w0zei2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = total_df[total_df[\"fName\"] == \"1109\"]\n",
    "iids = sample[\"input_ids\"]\n",
    "attn = sample[\"attention_mask\"]\n",
    "iids.values[0].shape, attn.values[0].shape, len(sample[\"tokenized_text\"].values[0])"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.375327Z",
     "end_time": "2023-04-15T20:59:46.436656Z"
    },
    "id": "2KLIPeG9zei3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "total_df.drop([\"entities\", \"text\", \"tokenizer_output\"], axis=1, inplace=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.382198Z",
     "end_time": "2023-04-15T20:59:46.436894Z"
    },
    "id": "HDaBlvzazei5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_df.reset_index(inplace=True)\n",
    "total_df.drop(\"index\", axis=1, inplace=True)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.640986Z",
     "end_time": "2023-04-15T20:59:46.671317Z"
    },
    "id": "_eTnmcmJzei8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test token mapping for the sample\n",
    "n = 25\n",
    "sample = total_df.iloc[n]\n",
    "sample_tokens = sample[\"tokenized_text\"]\n",
    "sample_entities = sample[\"classified_tokens\"]\n",
    "a = np.array([sample_tokens, sample_entities])\n",
    "a.T"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.773880Z",
     "end_time": "2023-04-15T20:59:46.797862Z"
    },
    "id": "A2zF_wsbzei9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_entities = []\n",
    "for sample in total_df[\"classified_tokens\"]:\n",
    "    for token in sample:\n",
    "        unique_entities.append(token)\n",
    "unique_entities = np.unique(unique_entities)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.774144Z",
     "end_time": "2023-04-15T20:59:46.851504Z"
    },
    "id": "RQFcjrnbzei-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "for i in range(len(unique_entities)):\n",
    "    id2label[i] = unique_entities[i]\n",
    "    label2id[unique_entities[i]] = i"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.852261Z",
     "end_time": "2023-04-15T20:59:46.894398Z"
    },
    "id": "2Bbs6bwqzei_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_num = len(unique_entities)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.894683Z",
     "end_time": "2023-04-15T20:59:46.895030Z"
    },
    "id": "bgXGy0ZAzei_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_num"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.895012Z",
     "end_time": "2023-04-15T20:59:46.898243Z"
    },
    "id": "y8hEO8qdzejA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model architecture\n",
    "class BertTokenClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertTokenClassification, self).__init__()\n",
    "        self.bert = AutoModelForTokenClassification.from_pretrained(\"Evolett/rubert-tiny2-finetuned-ner\")\n",
    "\n",
    "        self.bert.id2label = id2label\n",
    "        self.bert.label2id = label2id\n",
    "        self.bert.num_labels = labels_num\n",
    "        self.bert.classifier = torch.nn.Linear(312, labels_num)\n",
    "\n",
    "    def forward(self, input_id, mask, label=None):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label)\n",
    "        return output"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.895393Z",
     "end_time": "2023-04-15T20:59:46.898631Z"
    },
    "id": "GqZ9PrPqzejB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @staticmethod\n",
    "    def df_sample_to_tensor(row):\n",
    "        data = row[\"input_ids\"]\n",
    "        labels = row[\"classified_tokens\"]\n",
    "        labels = list(map(lambda x: label2id[x], labels))\n",
    "        labels = torch.tensor(labels)\n",
    "        data = torch.squeeze(data).int()\n",
    "        # print(labels.shape, data.shape)\n",
    "        return data, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df_sample_to_tensor(self.df.iloc[idx])"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.895679Z",
     "end_time": "2023-04-15T20:59:46.898758Z"
    },
    "id": "wZ0xkI3bzejC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# complement input ids and attention mask\n",
    "# to 2048 elements, to bring all tensors to the same shape\n",
    "def tokenization_complement(tensor):\n",
    "    complement_size = 2048 - tensor.shape[1]\n",
    "    if complement_size < 0:\n",
    "        return tensor[:, :2048]\n",
    "    zeros_complement = torch.zeros((1, complement_size))\n",
    "    new_sample = torch.cat(\n",
    "        (\n",
    "            tensor,\n",
    "            zeros_complement,\n",
    "        ),\n",
    "        dim=1,\n",
    "    )\n",
    "    assert new_sample.shape[1] == 2048\n",
    "    return new_sample\n",
    "\n",
    "\n",
    "# complement labels to the form of tensors\n",
    "def labels_complement(labels):\n",
    "    complement_size = 2048 - len(labels)\n",
    "    if complement_size < 0:\n",
    "        return labels[:2048]\n",
    "    new_labels = labels\n",
    "    for i in range(complement_size):\n",
    "        new_labels.append(\"O\")\n",
    "    return new_labels"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.896290Z",
     "end_time": "2023-04-15T20:59:46.898845Z"
    },
    "id": "qR3wMVMWzejC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_df[\"input_ids\"] = total_df[\"input_ids\"].apply(tokenization_complement)\n",
    "total_df[\"attention_mask\"] = total_df[\"attention_mask\"].apply(tokenization_complement)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.896607Z",
     "end_time": "2023-04-15T20:59:46.962621Z"
    },
    "id": "bAL3xjctzejD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_df[\"classified_tokens\"] = total_df[\"classified_tokens\"].apply(labels_complement)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.940750Z",
     "end_time": "2023-04-15T20:59:46.994083Z"
    },
    "id": "QXraje5tzejE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(total_df, train_size=0.85)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:46.985795Z",
     "end_time": "2023-04-15T20:59:47.169329Z"
    },
    "id": "O6HZJIRnzejF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "lr = 3e-4\n",
    "EPOCH_NUMBER = 10\n",
    "\n",
    "\n",
    "def train_loop(model, train_d, val_d, epoch_num=EPOCH_NUMBER):\n",
    "    train_dataset = TextDataset(train_d)\n",
    "    val_dataset = TextDataset(val_d)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        acc_train = 0\n",
    "        cur_loss_train = 0\n",
    "        acc_val = 0\n",
    "        loss_val = 0\n",
    "\n",
    "        model.train()\n",
    "        for data, labels in tqdm(train_dataloader):\n",
    "            attention_mask = (data != 0).float().to(device)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data, attention_mask, labels)\n",
    "            loss, logits = output[0], output[1]\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "                clean_logits = logits[i][labels[i] != 58]\n",
    "                clean_labels = labels[i][labels[i] != 58]\n",
    "                \n",
    "                predictions = clean_logits.argmax(dim=1)\n",
    "                acc = (predictions == clean_labels).float().mean()\n",
    "                acc_train += acc\n",
    "                cur_loss_train += loss.item()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        for data, labels in tqdm(val_dataloader):\n",
    "            attention_mask = (data != 0).float().to(device)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(data, attention_mask, labels)\n",
    "            loss, logits = output[0], output[1]\n",
    "            for i in range(logits.shape[0]):\n",
    "                clean_logits = logits[i][labels[i] != 58]\n",
    "                clean_labels = labels[i][labels[i] != 58]\n",
    "\n",
    "                predictions = clean_logits.argmax(dim=1)\n",
    "                acc = (predictions == clean_labels).float().mean()\n",
    "                acc_val += acc\n",
    "                loss_val += loss.item()\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch + 1} | train Loss: {cur_loss_train / len(train_d): .3f} | train Accuracy: {acc_train / len(train_d): .3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"val Loss: {loss_val / len(val_d): .3f} | val Accuracy: {acc_val / len(val_d): .3f}\"\n",
    "        )"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:47.214434Z",
     "end_time": "2023-04-15T20:59:47.214855Z"
    },
    "id": "ren0bHvAzejF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:47.214831Z",
     "end_time": "2023-04-15T20:59:47.215137Z"
    },
    "id": "7j7tmeuWzejG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertTokenClassification().to(device)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-15T20:59:47.215120Z",
     "end_time": "2023-04-15T20:59:48.652474Z"
    },
    "id": "IRxpxSv5zejH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loop(model, train_data, val_data)"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-08T22:53:15.769398Z",
     "end_time": "2023-04-08T22:53:16.643053Z"
    },
    "id": "D6YIW600zejH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# model.load_state_dict(torch.load('model.pth'))"
   ],
   "metadata": {
    "id": "ZY4UqTktoSLM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ],
   "metadata": {
    "id": "Sbz6DZN3V2WM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir = \"test_data/\""
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-08T22:53:01.293656Z",
     "end_time": "2023-04-08T22:53:01.337980Z"
    },
    "id": "6wiXcOA_zejI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = []\n",
    "f_names = []\n",
    "for file_name in os.listdir(test_dir):\n",
    "    file_path = test_dir + file_name\n",
    "    if file_name.split(\".\")[1] == \"txt\":\n",
    "        with open(file_path) as f:\n",
    "            cur_text = f.read()\n",
    "        texts.append(cur_text)\n",
    "        f_names.append(file_name)\n",
    "\n",
    "test_df = pd.DataFrame({\"fName\": f_names, \"text\": texts})"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-08T22:53:01.336762Z",
     "end_time": "2023-04-08T22:53:01.338118Z"
    },
    "id": "lgUYggwwzejJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_df[\"tokenized_text\"] = test_df[\"text\"].apply(tokenizer.tokenize)"
   ],
   "metadata": {
    "id": "-4v2asHkLyu_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df[\"tokenized_text\"] = test_df[\"tokenized_text\"].apply(\n",
    "    lambda x: [\"START\"] + x + [\"END\"]\n",
    ")"
   ],
   "metadata": {
    "id": "J2UEmHRGN9dy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df[\"tokenized_text\"].apply(len).hist(bins=50)"
   ],
   "metadata": {
    "id": "nF7ae5gOOmup"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df[\"tokenizer_output\"] = test_df[\"text\"].apply(\n",
    "    lambda x: tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
    ")"
   ],
   "metadata": {
    "id": "fSGrrN3rPSs1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_df"
   ],
   "metadata": {
    "id": "V1ELtguCS8bw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def model_inference(sample):\n",
    "    text = sample[\"text\"]\n",
    "    model_input = tokenizer(text)\n",
    "    input_len = len(model_input[\"input_ids\"])\n",
    "    input_ids = torch.tensor(model_input[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    # print(input_len)\n",
    "\n",
    "    attention_mask = torch.tensor(model_input[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    logits = output[0]\n",
    "    predictions = logits.argmax(dim=2).squeeze(0)\n",
    "\n",
    "    result = []\n",
    "    for prediction in predictions:\n",
    "        result.append(id2label[prediction.item()])\n",
    "\n",
    "    text_tokenized = sample[\"tokenized_text\"]\n",
    "    arr = np.vstack((text_tokenized, result))\n",
    "    arr = arr.T\n",
    "    # print(arr)\n",
    "\n",
    "    text = sample[\"text\"]\n",
    "    processed_tokens = 1\n",
    "    cur_token = arr[processed_tokens][0]\n",
    "    cur_label = arr[processed_tokens][1]\n",
    "    ans = []\n",
    "    for i in range(len(text)):\n",
    "        if cur_token == \"END\":\n",
    "            break\n",
    "\n",
    "        if cur_token == \"[UNK]\":\n",
    "            processed_tokens += 1\n",
    "            cur_token = arr[processed_tokens][0]\n",
    "            cur_label = arr[processed_tokens][1]\n",
    "\n",
    "        token_len = len(cur_token)\n",
    "        cur_token_fixed = cur_token\n",
    "\n",
    "        if cur_token[:2] == \"##\":\n",
    "            token_len -= 2\n",
    "            cur_token_fixed = cur_token[2:]\n",
    "\n",
    "        assert len(text[i : i + token_len]) == len(cur_token_fixed)\n",
    "        if len(text[i : i + token_len]) != len(cur_token_fixed):\n",
    "            print(text[i : i + token_len], \"--\", len(text[i : i + token_len]))\n",
    "            print(cur_token_fixed, \"--\", len(cur_token_fixed))\n",
    "            return\n",
    "\n",
    "        if text[i : i + token_len] == cur_token_fixed:\n",
    "            ans.append((i, i + token_len, cur_token, cur_label))\n",
    "            processed_tokens += 1\n",
    "            cur_token = arr[processed_tokens][0]\n",
    "            cur_label = arr[processed_tokens][1]\n",
    "\n",
    "    ans1 = []\n",
    "    i = 0\n",
    "    while i < len(ans):\n",
    "        label = ans[i][3]\n",
    "        if label[0] != \"O\":\n",
    "            total_word = ans[i][2]\n",
    "            begin = ans[i][0]\n",
    "            end = ans[i][1]\n",
    "            j = i + 1\n",
    "            while ans[j][3][0] == \"I\":\n",
    "                cur_word = ans[j][2]\n",
    "\n",
    "                if cur_word[0:2] != \"##\":\n",
    "                    cur_word = \" \" + cur_word\n",
    "                else:\n",
    "                    cur_word = cur_word[2:]\n",
    "\n",
    "                total_word += cur_word\n",
    "                end = ans[j][1]\n",
    "                j += 1\n",
    "            ans1.append((begin, end, total_word, label[2:]))\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return ans1"
   ],
   "metadata": {
    "id": "CmFj36WSUtey"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample = test_df.iloc[2]\n",
    "model_inference(sample)"
   ],
   "metadata": {
    "id": "IdubUFduWMpf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "file = open(\"answer.txt\", 'w')\n",
    "\n",
    "for i in range(test_df.shape[0]):\n",
    "    sample = test_df.iloc[i]\n",
    "    fname = sample['fName']\n",
    "    fname = fname.split('.')[0] + '.ann'\n",
    "    res = model_inference(sample)\n",
    "    file.write(fname + '\\n')\n",
    "    for elem in res:\n",
    "        begin = elem[0]\n",
    "        end = elem[1]\n",
    "        substr = elem[2]\n",
    "        label = elem[3]\n",
    "        result_str = label + ' ' +  str(begin) + ' ' + str(end) + '\\n'\n",
    "        file.write(result_str)\n",
    "\n",
    "file.close()"
   ],
   "metadata": {
    "id": "oRUtdmExyG79"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
