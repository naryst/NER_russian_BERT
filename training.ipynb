{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naryst/NER_russian_BERT/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1FbAhm8H7MtPN_CJj-kYuI7jy_CRQsuys"
      ],
      "metadata": {
        "id": "M1XHxJhZE2gD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -n data.zip "
      ],
      "metadata": {
        "id": "CmiFfnxTFHPs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet"
      ],
      "metadata": {
        "id": "e49naqIFFKZW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:37.020515Z",
          "end_time": "2023-04-15T20:59:40.672236Z"
        },
        "id": "2dL8xO4szeid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "ROOT_DIR = \"train_data/\"\n",
        "texts50 = ROOT_DIR + \"50texts_tacred/\"\n",
        "cll3 = ROOT_DIR + \"coll3_tacred/\"\n",
        "legal = ROOT_DIR + \"legal/\"\n",
        "tacred2 = ROOT_DIR + \"tacred2/\"\n",
        "tacred3 = ROOT_DIR + \"tacred3/\""
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:40.674242Z",
          "end_time": "2023-04-15T20:59:40.678166Z"
        },
        "id": "kalyOhTgzeih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "def create_dataframe(dir):\n",
        "    df = pd.DataFrame(columns=[\"fName\", \"text\", \"entities\"])\n",
        "    df = df.set_index(\"fName\")\n",
        "    files = os.listdir(dir)\n",
        "    for name in files:\n",
        "        fname, ext = name.split(\".\")\n",
        "        with open(dir + name) as f:\n",
        "            text = f.read()\n",
        "        if ext == \"txt\":\n",
        "            ann_fname = files[files.index(fname + \".ann\")]\n",
        "            with open(dir + ann_fname) as f_ann:\n",
        "                entities = f_ann.readlines()\n",
        "            new_entities = []\n",
        "            for i in range(len(entities)):\n",
        "                entity_type = entities[i][0]\n",
        "                entities[i] = entities[i][:-1]\n",
        "                if entity_type == \"T\":\n",
        "                    entities[i] = entities[i].split(\"\\t\")\n",
        "                    entities[i] = entities[i][1:]\n",
        "                    src = entities[i][1]\n",
        "                    entities[i] = entities[i][0].split()\n",
        "                    entities[i].append(src)\n",
        "                    assert len(entities[i]) <= 5\n",
        "                    if len(entities[i]) == 5:\n",
        "                        begin = entities[i][1]\n",
        "                        inter1, inter2 = entities[i][2].split(\";\")\n",
        "                        end = entities[i][3]\n",
        "                        new_entities.append(\n",
        "                            [entities[i][0], begin, inter1, entities[i][4]]\n",
        "                        )\n",
        "                        new_entities.append(\n",
        "                            [entities[i][0], inter2, end, entities[i][4]]\n",
        "                        )\n",
        "                    else:\n",
        "                        new_entities.append(entities[i])\n",
        "            df.loc[fname] = [text, new_entities]\n",
        "    return df"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:40.680428Z",
          "end_time": "2023-04-15T20:59:40.698538Z"
        },
        "id": "UlS-SEorzeij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.9/warnings.py\u001b[0m in \u001b[0;36m_add_filter\u001b[0;34m(append, *item)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fb25d5726b86>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcll3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtacred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtacred3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e85c4b75fab7>\u001b[0m in \u001b[0;36mcreate_dataframe\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mnew_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_entities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   9787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9788\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9789\u001b[0;31m             \u001b[0mrow_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9790\u001b[0m             \u001b[0;31m# infer_objects is needed for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9791\u001b[0m             \u001b[0;31m#  test_append_empty_frame_to_series_with_dateutil_tz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_frame\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m                 )\n\u001b[1;32m    571\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dtype_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36masarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# Can remove warning filter once NumPy 1.24 is min version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/warnings.py\u001b[0m in \u001b[0;36msimplefilter\u001b[0;34m(action, category, lineno, append)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m            \u001b[0;34m\"lineno must be an int >= 0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0m_add_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_add_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/warnings.py\u001b[0m in \u001b[0;36m_add_filter\u001b[0;34m(append, *item)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df1 = create_dataframe(texts50)\n",
        "df2 = create_dataframe(cll3)\n",
        "df3 = create_dataframe(legal)\n",
        "df4 = create_dataframe(tacred2)\n",
        "df5 = create_dataframe(tacred3)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:40.686734Z",
          "end_time": "2023-04-15T20:59:41.671342Z"
        },
        "id": "Y5dP52Fazeim",
        "outputId": "fd1e9d64-3c45-4a7a-dfc7-a9929422d1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "df1.shape[0] + df2.shape[0] + df3.shape[0] + df4.shape[0] + df5.shape[0]"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:41.675124Z",
          "end_time": "2023-04-15T20:59:41.680949Z"
        },
        "id": "Oo5nCp4izeio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df = pd.concat([df1, df2, df3, df4, df5])"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:41.679165Z",
          "end_time": "2023-04-15T20:59:41.691374Z"
        },
        "id": "0lggtpAfzeip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:41.689245Z",
          "end_time": "2023-04-15T20:59:41.782652Z"
        },
        "id": "5a1plEEozeir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# change the parameter of max tokens what model can process to get tokenization of the whole input\n",
        "# (not only first 512 tokens, which BERT can process  by its architecture)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     \"yqelz/xml-roberta-large-ner-russian\", model_max_length=int(1.5e4)\n",
        "# )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Evolett/rubert-tiny2-finetuned-ner\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:41.750418Z",
          "end_time": "2023-04-15T20:59:42.267701Z"
        },
        "id": "Cn5DkX3Xzeir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def identify_tokens(df_sample):\n",
        "    sample_text = df_sample[\"text\"]\n",
        "    sample_en = df_sample[\"entities\"]\n",
        "    sample_text_tokenized = tokenizer.tokenize(sample_text)\n",
        "    trash_symbols_count = []\n",
        "    for i in range(len(sample_text)):\n",
        "        cur = (\n",
        "            1\n",
        "            if (\n",
        "                sample_text[i] == \" \"\n",
        "                or sample_text[i] == \"\\n\"\n",
        "                or sample_text[i] == \"\\xa0\"\n",
        "            )\n",
        "            else 0\n",
        "        )\n",
        "        if i == 0:\n",
        "            trash_symbols_count.append(cur)\n",
        "        else:\n",
        "            trash_symbols_count.append(trash_symbols_count[-1] + cur)\n",
        "    cur_entity_id = 0\n",
        "    processed_symbols = 0\n",
        "    mappings = []\n",
        "    begin = int(sample_en[cur_entity_id][1])\n",
        "    end = int(sample_en[cur_entity_id][2])\n",
        "    assert end < len(trash_symbols_count)\n",
        "    begin -= trash_symbols_count[begin]\n",
        "    end -= trash_symbols_count[end]\n",
        "\n",
        "    for token in sample_text_tokenized:\n",
        "        while processed_symbols > end and (cur_entity_id + 1) < len(sample_en):\n",
        "            cur_entity_id += 1\n",
        "            begin = int(sample_en[cur_entity_id][1])\n",
        "            end = int(sample_en[cur_entity_id][2]) - 1\n",
        "            begin -= trash_symbols_count[begin]\n",
        "            end -= trash_symbols_count[end]\n",
        "        if begin <= processed_symbols <= end:\n",
        "            first_token = not (\n",
        "                len(mappings) != 0 and sample_en[cur_entity_id] == mappings[-1][1]\n",
        "            )\n",
        "            mappings.append((token, sample_en[cur_entity_id], first_token))\n",
        "        else:\n",
        "            mappings.append((token, False))\n",
        "        subword_check = token[:2] == '##'\n",
        "        processed_symbols = processed_symbols + len(token) - int(subword_check) * 2\n",
        "\n",
        "    for i in range(len(mappings)):\n",
        "        if len(mappings[i]) == 2:\n",
        "            mappings[i] = \"O\"\n",
        "        else:\n",
        "            if mappings[i][-1]:\n",
        "                mappings[i] = \"B-\" + mappings[i][1][0]\n",
        "            else:\n",
        "                mappings[i] = \"I-\" + mappings[i][1][0]\n",
        "    mappings.insert(0, \"O\")  # start token\n",
        "    mappings.append(\"O\")  # end token\n",
        "    return mappings"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:42.274856Z",
          "end_time": "2023-04-15T20:59:42.276541Z"
        },
        "id": "WJWc-zhKzeis"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df[\"tokenized_text\"] = total_df[\"text\"].apply(tokenizer.tokenize)\n",
        "total_df[\"tokenizer_output\"] = total_df[\"text\"].apply(\n",
        "    lambda x: tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        ")\n",
        "# add start and end tokens to map tokenizer output\n",
        "total_df[\"tokenized_text\"] = total_df[\"tokenized_text\"].apply(\n",
        "    lambda x: [\"<s>\"] + x + [\"</s>\"]\n",
        ")\n",
        "total_df[\"classified_tokens\"] = total_df.apply(identify_tokens, axis=1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:42.278761Z",
          "end_time": "2023-04-15T20:59:45.625728Z"
        },
        "id": "qbbneyatzeiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df.reset_index(inplace=True)\n",
        "total_df.head()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:45.626498Z",
          "end_time": "2023-04-15T20:59:45.700930Z"
        },
        "id": "PBeppkmIzeix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df[\"tokenized_text\"].apply(len).hist(bins=150)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:45.690447Z",
          "end_time": "2023-04-15T20:59:46.362073Z"
        },
        "id": "JcncVFK7zeiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "lens = total_df[\"classified_tokens\"].apply(len)\n",
        "print(\n",
        "    f\"Number of samples in train dataset, which have more tokens, when BERT can process - {lens[lens > 2048].count() / lens.count() * 100:.3f}%\"\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.365160Z",
          "end_time": "2023-04-15T20:59:46.366932Z"
        },
        "id": "yLTQLyGkzei0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset have 21% of samples, what can't be processed by BERT(For example, in document '1109' input text tokenized at more than 3000 tokens). To deal with this problem, we will use sliding window approach. We will split each sample into several samples, each of which will contain 512 tokens. We will use 256 tokens overlap between samples."
      ],
      "metadata": {
        "collapsed": false,
        "id": "S1Zc_5BMzei1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df[\"input_ids\"] = total_df[\"tokenizer_output\"].apply(lambda x: x[\"input_ids\"])\n",
        "total_df[\"attention_mask\"] = total_df[\"tokenizer_output\"].apply(\n",
        "    lambda x: x[\"attention_mask\"]\n",
        ")"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.369479Z",
          "end_time": "2023-04-15T20:59:46.374991Z"
        },
        "id": "hT7nK7w0zei2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "sample = total_df[total_df[\"fName\"] == \"1109\"]\n",
        "# sample = total_df.iloc[39]\n",
        "iids = sample[\"input_ids\"]\n",
        "attn = sample[\"attention_mask\"]\n",
        "iids.values[0].shape, attn.values[0].shape, len(sample[\"tokenized_text\"].values[0])"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.375327Z",
          "end_time": "2023-04-15T20:59:46.436656Z"
        },
        "id": "2KLIPeG9zei3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# drop useless columns\n",
        "total_df.drop([\"entities\", \"text\", \"tokenizer_output\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.382198Z",
          "end_time": "2023-04-15T20:59:46.436894Z"
        },
        "id": "HDaBlvzazei5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.434044Z",
          "end_time": "2023-04-15T20:59:46.671005Z"
        },
        "id": "FgwS-jCczei6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# total_df = pd.concat(total_df.values)\n",
        "total_df.reset_index(inplace=True)\n",
        "total_df.drop(\"index\", axis=1, inplace=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.640986Z",
          "end_time": "2023-04-15T20:59:46.671317Z"
        },
        "id": "_eTnmcmJzei8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# test token mapping for the sample\n",
        "n = 25\n",
        "sample = total_df.iloc[n]\n",
        "sample_tokens = sample[\"tokenized_text\"]\n",
        "sample_entities = sample[\"classified_tokens\"]\n",
        "a = np.array([sample_tokens, sample_entities])\n",
        "a.T"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.773880Z",
          "end_time": "2023-04-15T20:59:46.797862Z"
        },
        "id": "A2zF_wsbzei9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "unique_entities = []\n",
        "for sample in total_df[\"classified_tokens\"]:\n",
        "    for token in sample:\n",
        "        unique_entities.append(token)\n",
        "unique_entities = np.unique(unique_entities)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.774144Z",
          "end_time": "2023-04-15T20:59:46.851504Z"
        },
        "id": "RQFcjrnbzei-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "id2label = {}\n",
        "label2id = {}\n",
        "\n",
        "for i in range(len(unique_entities)):\n",
        "    id2label[i] = unique_entities[i]\n",
        "    label2id[unique_entities[i]] = i"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.852261Z",
          "end_time": "2023-04-15T20:59:46.894398Z"
        },
        "id": "2Bbs6bwqzei_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "labels_num = len(unique_entities)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.894683Z",
          "end_time": "2023-04-15T20:59:46.895030Z"
        },
        "id": "bgXGy0ZAzei_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "labels_num"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.895012Z",
          "end_time": "2023-04-15T20:59:46.898243Z"
        },
        "id": "y8hEO8qdzejA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class BertTokenClassification(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertTokenClassification, self).__init__()\n",
        "        self.bert = AutoModelForTokenClassification.from_pretrained(\"Evolett/rubert-tiny2-finetuned-ner\")\n",
        "\n",
        "        self.bert.id2label = id2label\n",
        "        self.bert.label2id = label2id\n",
        "        self.bert.num_labels = labels_num\n",
        "        self.bert.classifier = torch.nn.Linear(312, labels_num)\n",
        "\n",
        "    def forward(self, input_id, mask, label=None):\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label)\n",
        "        return output"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.895393Z",
          "end_time": "2023-04-15T20:59:46.898631Z"
        },
        "id": "GqZ9PrPqzejB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.df = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    @staticmethod\n",
        "    def df_sample_to_tensor(row):\n",
        "        data = row[\"input_ids\"]\n",
        "        labels = row[\"classified_tokens\"]\n",
        "        labels = list(map(lambda x: label2id[x], labels))\n",
        "        labels = torch.tensor(labels)\n",
        "        data = torch.squeeze(data).int()\n",
        "        # print(labels.shape, data.shape)\n",
        "        return data, labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.df_sample_to_tensor(self.df.iloc[idx])"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.895679Z",
          "end_time": "2023-04-15T20:59:46.898758Z"
        },
        "id": "wZ0xkI3bzejC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# complement input ids and attention mask\n",
        "# to 2048 elements, to bring all tensors to the same shape\n",
        "def tokenization_complement(tensor):\n",
        "    complement_size = 2048 - tensor.shape[1]\n",
        "    if complement_size < 0:\n",
        "        return tensor[:, :2048]\n",
        "    zeros_complement = torch.zeros((1, complement_size))\n",
        "    new_sample = torch.cat(\n",
        "        (\n",
        "            tensor,\n",
        "            zeros_complement,\n",
        "        ),\n",
        "        dim=1,\n",
        "    )\n",
        "    assert new_sample.shape[1] == 2048\n",
        "    return new_sample\n",
        "\n",
        "\n",
        "# complement labels to the form of tensors\n",
        "def labels_complement(labels):\n",
        "    complement_size = 2048 - len(labels)\n",
        "    if complement_size < 0:\n",
        "        return labels[:2048]\n",
        "    new_labels = labels\n",
        "    for i in range(complement_size):\n",
        "        new_labels.append(\"O\")\n",
        "    return new_labels"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.896290Z",
          "end_time": "2023-04-15T20:59:46.898845Z"
        },
        "id": "qR3wMVMWzejC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df[\"input_ids\"] = total_df[\"input_ids\"].apply(tokenization_complement)\n",
        "total_df[\"attention_mask\"] = total_df[\"attention_mask\"].apply(tokenization_complement)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.896607Z",
          "end_time": "2023-04-15T20:59:46.962621Z"
        },
        "id": "bAL3xjctzejD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "total_df[\"classified_tokens\"] = total_df[\"classified_tokens\"].apply(labels_complement)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.940750Z",
          "end_time": "2023-04-15T20:59:46.994083Z"
        },
        "id": "QXraje5tzejE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(total_df, train_size=0.85)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:46.985795Z",
          "end_time": "2023-04-15T20:59:47.169329Z"
        },
        "id": "O6HZJIRnzejF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "lr = 3e-4\n",
        "EPOCH_NUMBER = 10\n",
        "\n",
        "\n",
        "def train_loop(model, train_d, val_d, epoch_num=EPOCH_NUMBER):\n",
        "    train_dataset = TextDataset(train_d)\n",
        "    val_dataset = TextDataset(val_d)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epoch_num):\n",
        "        acc_train = 0\n",
        "        cur_loss_train = 0\n",
        "        acc_val = 0\n",
        "        loss_val = 0\n",
        "\n",
        "        model.train()\n",
        "        for data, labels in tqdm(train_dataloader):\n",
        "            attention_mask = (data != 0).float().to(device)\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # print(data.shape)\n",
        "            # print(labels.shape)\n",
        "\n",
        "            output = model(data, attention_mask, labels)\n",
        "            loss, logits = output[0], output[1]\n",
        "            # print(logits.shape)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "                clean_logits = logits[i][labels[i] != 58]\n",
        "                clean_labels = labels[i][labels[i] != 58]\n",
        "                \n",
        "                predictions = clean_logits.argmax(dim=1)\n",
        "                acc = (predictions == clean_labels).float().mean()\n",
        "                acc_train += acc\n",
        "                cur_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        for data, labels in tqdm(val_dataloader):\n",
        "            attention_mask = (data != 0).float().to(device)\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = model(data, attention_mask, labels)\n",
        "            loss, logits = output[0], output[1]\n",
        "            for i in range(logits.shape[0]):\n",
        "                clean_logits = logits[i][labels[i] != 58]\n",
        "                clean_labels = labels[i][labels[i] != 58]\n",
        "\n",
        "                predictions = clean_logits.argmax(dim=1)\n",
        "                acc = (predictions == clean_labels).float().mean()\n",
        "                acc_val += acc\n",
        "                loss_val += loss.item()\n",
        "\n",
        "        print(\n",
        "            f\"Epochs: {epoch + 1} | train Loss: {cur_loss_train / len(train_d): .3f} | train Accuracy: {acc_train / len(train_d): .3f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"val Loss: {loss_val / len(val_d): .3f} | val Accuracy: {acc_val / len(val_d): .3f}\"\n",
        "        )\n",
        "        # scheduler.step(loss_val)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:47.214434Z",
          "end_time": "2023-04-15T20:59:47.214855Z"
        },
        "id": "ren0bHvAzejF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:47.214831Z",
          "end_time": "2023-04-15T20:59:47.215137Z"
        },
        "id": "7j7tmeuWzejG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = BertTokenClassification().to(device)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-15T20:59:47.215120Z",
          "end_time": "2023-04-15T20:59:48.652474Z"
        },
        "id": "IRxpxSv5zejH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_loop(model, train_data, val_data)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-08T22:53:15.769398Z",
          "end_time": "2023-04-08T22:53:16.643053Z"
        },
        "id": "D6YIW600zejH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('model.pth'))"
      ],
      "metadata": {
        "id": "ZY4UqTktoSLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "Sbz6DZN3V2WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_dir = \"test_data/\""
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-08T22:53:01.293656Z",
          "end_time": "2023-04-08T22:53:01.337980Z"
        },
        "id": "6wiXcOA_zejI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "texts = []\n",
        "f_names = []\n",
        "for file_name in os.listdir(test_dir):\n",
        "    file_path = test_dir + file_name\n",
        "    # print(file_name)\n",
        "    if file_name.split(\".\")[1] == \"txt\":\n",
        "        with open(file_path) as f:\n",
        "            cur_text = f.read()\n",
        "        texts.append(cur_text)\n",
        "        f_names.append(file_name)\n",
        "\n",
        "test_df = pd.DataFrame({\"fName\": f_names, \"text\": texts})"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2023-04-08T22:53:01.336762Z",
          "end_time": "2023-04-08T22:53:01.338118Z"
        },
        "id": "lgUYggwwzejJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"tokenized_text\"] = test_df[\"text\"].apply(tokenizer.tokenize)"
      ],
      "metadata": {
        "id": "-4v2asHkLyu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"tokenized_text\"] = test_df[\"tokenized_text\"].apply(\n",
        "    lambda x: [\"START\"] + x + [\"END\"]\n",
        ")"
      ],
      "metadata": {
        "id": "J2UEmHRGN9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"tokenized_text\"].apply(len).hist(bins=50)"
      ],
      "metadata": {
        "id": "nF7ae5gOOmup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['text'] = test_df['text'].apply(lambda x: x.replace('\\xa0', ' '))\n",
        "# test_df['text'] = test_df['text'].apply(lambda x: x.replace(' â€” ', ' '))"
      ],
      "metadata": {
        "id": "-rJCi4O7vvV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"tokenizer_output\"] = test_df[\"text\"].apply(\n",
        "    lambda x: tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True)\n",
        ")"
      ],
      "metadata": {
        "id": "fSGrrN3rPSs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "V1ELtguCS8bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_inference(sample):\n",
        "    text = sample[\"text\"]\n",
        "    model_input = tokenizer(text)\n",
        "    input_len = len(model_input[\"input_ids\"])\n",
        "    input_ids = torch.tensor(model_input[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "    # print(input_len)\n",
        "\n",
        "    attention_mask = torch.tensor(model_input[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "    output = model(input_ids, attention_mask)\n",
        "    logits = output[0]\n",
        "    predictions = logits.argmax(dim=2).squeeze(0)\n",
        "\n",
        "    result = []\n",
        "    for prediction in predictions:\n",
        "        result.append(id2label[prediction.item()])\n",
        "\n",
        "    text_tokenized = sample[\"tokenized_text\"]\n",
        "    arr = np.vstack((text_tokenized, result))\n",
        "    arr = arr.T\n",
        "    # print(arr)\n",
        "\n",
        "    text = sample[\"text\"]\n",
        "    processed_tokens = 1\n",
        "    cur_token = arr[processed_tokens][0]\n",
        "    cur_label = arr[processed_tokens][1]\n",
        "    ans = []\n",
        "    for i in range(len(text)):\n",
        "        if cur_token == \"END\":\n",
        "            break\n",
        "\n",
        "        if cur_token == \"[UNK]\":\n",
        "            processed_tokens += 1\n",
        "            cur_token = arr[processed_tokens][0]\n",
        "            cur_label = arr[processed_tokens][1]\n",
        "\n",
        "        token_len = len(cur_token)\n",
        "        cur_token_fixed = cur_token\n",
        "\n",
        "        if cur_token[:2] == \"##\":\n",
        "            token_len -= 2\n",
        "            cur_token_fixed = cur_token[2:]\n",
        "\n",
        "        assert len(text[i : i + token_len]) == len(cur_token_fixed)\n",
        "        if len(text[i : i + token_len]) != len(cur_token_fixed):\n",
        "            print(text[i : i + token_len], \"--\", len(text[i : i + token_len]))\n",
        "            print(cur_token_fixed, \"--\", len(cur_token_fixed))\n",
        "            return\n",
        "\n",
        "        if text[i : i + token_len] == cur_token_fixed:\n",
        "            ans.append((i, i + token_len, cur_token, cur_label))\n",
        "            processed_tokens += 1\n",
        "            cur_token = arr[processed_tokens][0]\n",
        "            cur_label = arr[processed_tokens][1]\n",
        "\n",
        "    ans1 = []\n",
        "    i = 0\n",
        "    while i < len(ans):\n",
        "        label = ans[i][3]\n",
        "        if label[0] != \"O\":\n",
        "            total_word = ans[i][2]\n",
        "            begin = ans[i][0]\n",
        "            end = ans[i][1]\n",
        "            j = i + 1\n",
        "            while ans[j][3][0] == \"I\":\n",
        "                cur_word = ans[j][2]\n",
        "\n",
        "                if cur_word[0:2] != \"##\":\n",
        "                    cur_word = \" \" + cur_word\n",
        "                else:\n",
        "                    cur_word = cur_word[2:]\n",
        "\n",
        "                total_word += cur_word\n",
        "                end = ans[j][1]\n",
        "                j += 1\n",
        "            ans1.append((begin, end, total_word, label[2:]))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return ans1"
      ],
      "metadata": {
        "id": "CmFj36WSUtey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = test_df.iloc[2]\n",
        "model_inference(sample)"
      ],
      "metadata": {
        "id": "IdubUFduWMpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"answer.txt\", 'w')\n",
        "\n",
        "for i in range(test_df.shape[0]):\n",
        "    sample = test_df.iloc[i]\n",
        "    fname = sample['fName']\n",
        "    fname = fname.split('.')[0] + '.ann'\n",
        "    res = model_inference(sample)\n",
        "    file.write(fname + '\\n')\n",
        "    for elem in res:\n",
        "        begin = elem[0]\n",
        "        end = elem[1]\n",
        "        substr = elem[2]\n",
        "        label = elem[3]\n",
        "        result_str = label + ' ' +  str(begin) + ' ' + str(end) + '\\n'\n",
        "        file.write(result_str)\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "oRUtdmExyG79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}